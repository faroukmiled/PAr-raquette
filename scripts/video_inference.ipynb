{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "INzAGQMw3YY0"
      },
      "source": [
        "# Roboflow Video Inference\n",
        "\n",
        "This colab notebook reflects [the Roboflow video inference repo](https://github.com/roboflow-ai/video-inference) but utilizes the pip package for inference calls instead of `CURL` commands.\n",
        "\n",
        "Documentation for the Roboflow pip package can be found [via this link](https://docs.roboflow.com/python).\n",
        "\n",
        "Video inference uses the following steps:\n",
        "- break down videos to images using FFMPEG\n",
        "- perform inference on each image and render a bounding box for each detected image\n",
        "- stich images back together into a video format"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gT6dn6Gl7WpI"
      },
      "source": [
        "# FFMPEG Set Up"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MFF8tkyK70_D"
      },
      "source": [
        "### FFMPEG Installation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 81,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "0c-2lgoHDiRp",
        "outputId": "af3b8e91-3d4f-462a-832e-e978c9cb14ea"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Installation finished.\n"
          ]
        }
      ],
      "source": [
        "from IPython.display import clear_output\n",
        "import os, urllib.request\n",
        "HOME = os.path.expanduser(\"~\")\n",
        "pathDoneCMD = f'{HOME}/doneCMD.sh'\n",
        "if not os.path.exists(f\"{HOME}/.ipython/ttmg.py\"):\n",
        "    hCode = \"https://raw.githubusercontent.com/yunooooo/gcct/master/res/ttmg.py\"\n",
        "    urllib.request.urlretrieve(hCode, f\"{HOME}/.ipython/ttmg.py\")\n",
        "\n",
        "from ttmg import (\n",
        "    loadingAn,\n",
        "    textAn,\n",
        ")\n",
        "\n",
        "loadingAn(name=\"lds\")\n",
        "textAn(\"Installing Dependencies...\", ty='twg')\n",
        "os.system('pip install git+git://github.com/AWConant/jikanpy.git')\n",
        "os.system('add-apt-repository -y ppa:jonathonf/ffmpeg-4')\n",
        "os.system('apt-get update')\n",
        "os.system('apt install mediainfo')\n",
        "os.system('apt-get install ffmpeg')\n",
        "clear_output()\n",
        "print('Installation finished.')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-vopNP4y8HXw"
      },
      "source": [
        "### FFMPEG Execution\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Access video\n",
        "\n",
        "Upload one video at a time for now to the `videos_to_infer` directory, video globs will result in videos being stiched together.\n",
        "\n",
        "You can connect g-drive and use `cp` to copy a single file out of it or manually upload a video from your local device.\n",
        "\n",
        "Regardless, make sure it lands in the `videos_to_infer` directory."
      ],
      "metadata": {
        "id": "iReTCPV9ET_8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/\n",
        "!mkdir videos_to_infer\n",
        "!mkdir inferred_videos\n",
        "%cd videos_to_infer"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C8ASYbq3oBoU",
        "outputId": "e7b99c20-203d-4e61-cbb3-e1f291017210"
      },
      "execution_count": 82,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content\n",
            "/content/videos_to_infer\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Optional: Link your Google Drive to upload files to/from Google Drive\n",
        "\n",
        "\n",
        "*   process outlined in the next 2 cells\n",
        "\n"
      ],
      "metadata": {
        "id": "I_pqrClDlEHy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# OPTIONAL - link your g-drive to pull videos from\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "metadata": {
        "id": "e15g_5kXiChj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# OPTIONAL - copy your videos from g-drive to /content/\n",
        "!cp /content/gdrive/YOUR_VIDEO.mp4 /content/videos_to_images"
      ],
      "metadata": {
        "id": "NvBwDrfjiKgc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Break down video frames into images"
      ],
      "metadata": {
        "id": "bjhxbtGXlW4t"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 85,
      "metadata": {
        "id": "SndB9usRtjW4"
      },
      "outputs": [],
      "source": [
        "# break video down into images - UPDATE THE PATH TO THE FILE!\n",
        "# Example: video file named 'Test.mp4'\n",
        "# (cont'd) update path to: \"/content/videos_to_infer/Test.mp4\"\n",
        "os.environ['inputFile'] = '/content/videos_to_infer/match_Trim.mp4'\n",
        "\n",
        "# fps value: the number of frames to sample per second from the video\n",
        "# higher value for fps: sample more frames\n",
        "!ffmpeg  -hide_banner -loglevel error -i \"$inputFile\" -vf fps=10 \"$inputFile_out%04d.png\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SINB3TC84VeD"
      },
      "source": [
        "# Video Inference Section"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Roboflow PIP package installation"
      ],
      "metadata": {
        "id": "XMQROKNEzK0_"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 86,
      "metadata": {
        "id": "r0sUmu8M4R9H",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "63d0ad65-cd94-49c2-8eec-827bae762ce2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: roboflow in /usr/local/lib/python3.7/dist-packages (0.2.20)\n",
            "Requirement already satisfied: Pillow>=7.1.2 in /usr/local/lib/python3.7/dist-packages (from roboflow) (7.1.2)\n",
            "Requirement already satisfied: tqdm>=4.41.0 in /usr/local/lib/python3.7/dist-packages (from roboflow) (4.64.1)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.7/dist-packages (from roboflow) (1.4.4)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from roboflow) (2.28.1)\n",
            "Requirement already satisfied: python-dateutil in /usr/local/lib/python3.7/dist-packages (from roboflow) (2.8.2)\n",
            "Requirement already satisfied: pyparsing==2.4.7 in /usr/local/lib/python3.7/dist-packages (from roboflow) (2.4.7)\n",
            "Requirement already satisfied: urllib3==1.26.6 in /usr/local/lib/python3.7/dist-packages (from roboflow) (1.26.6)\n",
            "Requirement already satisfied: certifi==2021.5.30 in /usr/local/lib/python3.7/dist-packages (from roboflow) (2021.5.30)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from roboflow) (1.15.0)\n",
            "Requirement already satisfied: opencv-python-headless>=4.5.1.48 in /usr/local/lib/python3.7/dist-packages (from roboflow) (4.6.0.66)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.7/dist-packages (from roboflow) (3.2.2)\n",
            "Requirement already satisfied: wget in /usr/local/lib/python3.7/dist-packages (from roboflow) (3.2)\n",
            "Requirement already satisfied: glob2 in /usr/local/lib/python3.7/dist-packages (from roboflow) (0.7)\n",
            "Requirement already satisfied: PyYAML>=5.3.1 in /usr/local/lib/python3.7/dist-packages (from roboflow) (6.0)\n",
            "Requirement already satisfied: idna==2.10 in /usr/local/lib/python3.7/dist-packages (from roboflow) (2.10)\n",
            "Requirement already satisfied: python-dotenv in /usr/local/lib/python3.7/dist-packages (from roboflow) (0.21.0)\n",
            "Requirement already satisfied: requests-toolbelt in /usr/local/lib/python3.7/dist-packages (from roboflow) (0.10.1)\n",
            "Requirement already satisfied: cycler==0.10.0 in /usr/local/lib/python3.7/dist-packages (from roboflow) (0.10.0)\n",
            "Requirement already satisfied: chardet==4.0.0 in /usr/local/lib/python3.7/dist-packages (from roboflow) (4.0.0)\n",
            "Requirement already satisfied: numpy>=1.18.5 in /usr/local/lib/python3.7/dist-packages (from roboflow) (1.21.6)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from kiwisolver>=1.3.1->roboflow) (4.1.1)\n",
            "Requirement already satisfied: charset-normalizer<3,>=2 in /usr/local/lib/python3.7/dist-packages (from requests->roboflow) (2.1.1)\n"
          ]
        }
      ],
      "source": [
        "!pip3 install roboflow"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BJ-brq5-3jiW"
      },
      "source": [
        "## Imports\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 87,
      "metadata": {
        "id": "Ff1UPGhO3R5V"
      },
      "outputs": [],
      "source": [
        "from roboflow import Roboflow\n",
        "import json\n",
        "from time import sleep\n",
        "from PIL import Image, ImageDraw\n",
        "import io\n",
        "import base64\n",
        "import requests\n",
        "from os.path import exists\n",
        "import os, sys, re, glob"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4hQFw965339z"
      },
      "source": [
        "## Initialization\n",
        "\n",
        "- private api key found in Roboflow > YOUR_WORKSPACE > Roboflow API\n",
        "- NOTE: this is your private key, not publishable key!\n",
        "\n",
        "**Having trouble finding your API key, version number or project ID?** The [documentation's quick start section](https://docs.roboflow.com/python) demostrates how you can find these via the Roboflow platform UI.\n",
        "* [Obtaining Your API Key](https://docs.roboflow.com/rest-api#obtaining-your-api-key)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 88,
      "metadata": {
        "id": "v-SafC0j3izF",
        "outputId": "7b291d0a-de1e-4eb2-ad15-9bd6884b0220",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loading Roboflow workspace...\n",
            "loading Roboflow project...\n",
            "{\n",
            "  \"id\": \"raquette/1\",\n",
            "  \"name\": \"Raquette \",\n",
            "  \"version\": \"1\",\n",
            "  \"classes\": null,\n",
            "  \"overlap\": 30,\n",
            "  \"confidence\": 40,\n",
            "  \"stroke\": 1,\n",
            "  \"labels\": false,\n",
            "  \"format\": \"json\",\n",
            "  \"base_url\": \"https://detect.roboflow.com/\"\n",
            "}\n"
          ]
        }
      ],
      "source": [
        "# workspace code\n",
        "from roboflow import Roboflow\n",
        "import json\n",
        "\n",
        "rf = Roboflow(api_key=\"6mmosvNzOLTgkbtR1Of4\")\n",
        "project = rf.workspace().project(\"raquette\")\n",
        "dataset = project.version(1)\n",
        "\n",
        "# grab the model from that project's version\n",
        "model = project.version(1).model\n",
        "print(model)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# HELPER FUNCTIONS BLOCK\n",
        "def draw_boxes(box, x0, y0, img, class_name):\n",
        "    # OPTIONAL - color map, change the key-values for each color to make the\n",
        "    # class output labels specific to your dataset\n",
        "    color_map = {\n",
        "        \"raquette_noire_Flore\":\"black\",\n",
        "        \"class2\":\"red\",\n",
        "    }\n",
        "\n",
        "    # get position coordinates\n",
        "    bbox = ImageDraw.Draw(img) \n",
        "\n",
        "    bbox.rectangle(box, outline =color_map[class_name], width=5)\n",
        "    bbox.text((x0, y0), class_name, fill='black', anchor='mm')\n",
        "\n",
        "    return img\n",
        "\n",
        "def save_with_bbox_renders(img):\n",
        "    file_name = os.path.basename(img.filename)\n",
        "    img.save('/content/inferred_videos/' + file_name)"
      ],
      "metadata": {
        "id": "F8Qo4j5cmWMw"
      },
      "execution_count": 99,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Execution"
      ],
      "metadata": {
        "id": "2CkY3Z7UzWnz"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 100,
      "metadata": {
        "id": "v613kCJOUcor",
        "outputId": "14ef8d94-f712-498f-b23b-72135afac878",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/inferred_videos\n",
            "/content/inferred_videos\n",
            "{'x': 754.5, 'y': 537.0, 'width': 15, 'height': 42, 'class': 'raquette_noire_Flore', 'confidence': 0.63, 'image_path': '/content/videos_to_infer/0001.png', 'prediction_type': 'ObjectDetectionModel'}\n",
            "{'x': 709.5, 'y': 573.0, 'width': 15, 'height': 36, 'class': 'raquette_noire_Flore', 'confidence': 0.61, 'image_path': '/content/videos_to_infer/0006.png', 'prediction_type': 'ObjectDetectionModel'}\n",
            "{'x': 780.0, 'y': 667.5, 'width': 30, 'height': 57, 'class': 'raquette_noire_Flore', 'confidence': 0.637, 'image_path': '/content/videos_to_infer/0028.png', 'prediction_type': 'ObjectDetectionModel'}\n",
            "{'x': 792.0, 'y': 664.5, 'width': 30, 'height': 57, 'class': 'raquette_noire_Flore', 'confidence': 0.591, 'image_path': '/content/videos_to_infer/0029.png', 'prediction_type': 'ObjectDetectionModel'}\n",
            "{'x': 796.5, 'y': 658.5, 'width': 27, 'height': 51, 'class': 'raquette_noire_Flore', 'confidence': 0.55, 'image_path': '/content/videos_to_infer/0030.png', 'prediction_type': 'ObjectDetectionModel'}\n",
            "{'x': 795.0, 'y': 649.5, 'width': 30, 'height': 51, 'class': 'raquette_noire_Flore', 'confidence': 0.58, 'image_path': '/content/videos_to_infer/0031.png', 'prediction_type': 'ObjectDetectionModel'}\n",
            "{'x': 796.5, 'y': 645.0, 'width': 33, 'height': 48, 'class': 'raquette_noire_Flore', 'confidence': 0.516, 'image_path': '/content/videos_to_infer/0032.png', 'prediction_type': 'ObjectDetectionModel'}\n",
            "{'x': 796.5, 'y': 646.5, 'width': 33, 'height': 51, 'class': 'raquette_noire_Flore', 'confidence': 0.573, 'image_path': '/content/videos_to_infer/0033.png', 'prediction_type': 'ObjectDetectionModel'}\n",
            "{'x': 796.5, 'y': 646.5, 'width': 33, 'height': 51, 'class': 'raquette_noire_Flore', 'confidence': 0.624, 'image_path': '/content/videos_to_infer/0034.png', 'prediction_type': 'ObjectDetectionModel'}\n",
            "{'x': 793.5, 'y': 640.5, 'width': 33, 'height': 51, 'class': 'raquette_noire_Flore', 'confidence': 0.668, 'image_path': '/content/videos_to_infer/0035.png', 'prediction_type': 'ObjectDetectionModel'}\n",
            "{'x': 793.5, 'y': 636.0, 'width': 33, 'height': 48, 'class': 'raquette_noire_Flore', 'confidence': 0.64, 'image_path': '/content/videos_to_infer/0036.png', 'prediction_type': 'ObjectDetectionModel'}\n",
            "{'x': 795.0, 'y': 636.0, 'width': 30, 'height': 48, 'class': 'raquette_noire_Flore', 'confidence': 0.638, 'image_path': '/content/videos_to_infer/0037.png', 'prediction_type': 'ObjectDetectionModel'}\n",
            "{'x': 795.0, 'y': 636.0, 'width': 30, 'height': 48, 'class': 'raquette_noire_Flore', 'confidence': 0.637, 'image_path': '/content/videos_to_infer/0038.png', 'prediction_type': 'ObjectDetectionModel'}\n",
            "{'x': 795.0, 'y': 636.0, 'width': 30, 'height': 48, 'class': 'raquette_noire_Flore', 'confidence': 0.642, 'image_path': '/content/videos_to_infer/0039.png', 'prediction_type': 'ObjectDetectionModel'}\n",
            "{'x': 795.0, 'y': 636.0, 'width': 30, 'height': 48, 'class': 'raquette_noire_Flore', 'confidence': 0.669, 'image_path': '/content/videos_to_infer/0040.png', 'prediction_type': 'ObjectDetectionModel'}\n",
            "{'x': 793.5, 'y': 642.0, 'width': 33, 'height': 48, 'class': 'raquette_noire_Flore', 'confidence': 0.669, 'image_path': '/content/videos_to_infer/0041.png', 'prediction_type': 'ObjectDetectionModel'}\n",
            "{'x': 783.0, 'y': 670.5, 'width': 30, 'height': 51, 'class': 'raquette_noire_Flore', 'confidence': 0.581, 'image_path': '/content/videos_to_infer/0042.png', 'prediction_type': 'ObjectDetectionModel'}\n",
            "{'x': 750.0, 'y': 376.5, 'width': 24, 'height': 39, 'class': 'raquette_noire_Flore', 'confidence': 0.663, 'image_path': '/content/videos_to_infer/0051.png', 'prediction_type': 'ObjectDetectionModel'}\n",
            "{'x': 709.5, 'y': 330.0, 'width': 21, 'height': 48, 'class': 'raquette_noire_Flore', 'confidence': 0.642, 'image_path': '/content/videos_to_infer/0052.png', 'prediction_type': 'ObjectDetectionModel'}\n",
            "{'x': 637.5, 'y': 325.5, 'width': 21, 'height': 51, 'class': 'raquette_noire_Flore', 'confidence': 0.622, 'image_path': '/content/videos_to_infer/0054.png', 'prediction_type': 'ObjectDetectionModel'}\n",
            "{'x': 604.5, 'y': 333.0, 'width': 27, 'height': 48, 'class': 'raquette_noire_Flore', 'confidence': 0.598, 'image_path': '/content/videos_to_infer/0055.png', 'prediction_type': 'ObjectDetectionModel'}\n",
            "{'x': 570.0, 'y': 352.5, 'width': 24, 'height': 39, 'class': 'raquette_noire_Flore', 'confidence': 0.557, 'image_path': '/content/videos_to_infer/0056.png', 'prediction_type': 'ObjectDetectionModel'}\n",
            "{'x': 538.5, 'y': 384.0, 'width': 27, 'height': 42, 'class': 'raquette_noire_Flore', 'confidence': 0.402, 'image_path': '/content/videos_to_infer/0057.png', 'prediction_type': 'ObjectDetectionModel'}\n"
          ]
        }
      ],
      "source": [
        "# perform inference on each image from the split up video\n",
        "\n",
        "%cd /content/inferred_videos\n",
        "!pwd\n",
        "# glob config values\n",
        "file_path = \"/content/videos_to_infer/\"\n",
        "extention = \".png\"\n",
        "\n",
        "# glob files based on location and file format\n",
        "globbed_files = sorted(glob.glob(file_path + '*' + extention))\n",
        "#print(globbed_files)\n",
        "\n",
        "for image in globbed_files:\n",
        "  # INFERENCE\n",
        "  predictions = model.predict(image).json()['predictions']\n",
        "  newly_rendered_image = Image.open(image)\n",
        "\n",
        "  # RENDER \n",
        "  # for each detection, create a crop and convert into CLIP encoding\n",
        "  for prediction in predictions:\n",
        "      print(prediction)\n",
        "      # rip bounding box coordinates from current detection\n",
        "      # note: infer returns center points of box as (x,y) and width, height\n",
        "      # ----- but pillow crop requires the top left and bottom right points to crop\n",
        "      x0 = prediction['x'] - prediction['width'] / 2\n",
        "      x1 = prediction['x'] + prediction['width'] / 2\n",
        "      y0 = prediction['y'] - prediction['height'] / 2\n",
        "      y1 = prediction['y'] + prediction['height'] / 2\n",
        "      box = (x0, y0, x1, y1)\n",
        "      newly_rendered_image = draw_boxes(box, x0, y0, newly_rendered_image, prediction['class'])\n",
        "\n",
        "  # WRITE\n",
        "  save_with_bbox_renders(newly_rendered_image)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "You must line up the starting file of your video (i.e. the image you want to start with) to the FFMPEG wildcard syntax.\n",
        "\n",
        "For example, if you video file is named `out0001.png` use `y%04d.png`.\n",
        "\n",
        "If your file is named `/content/inferred_videos/Pi_test_video.mov_out0001.png` use `/content/inferred_videos/Pi_test_video.mov_outy%04d.png`.\n",
        "\n"
      ],
      "metadata": {
        "id": "f_99RmVTzc67"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# stich images together into video\n",
        "!pwd\n",
        "!ffmpeg -r 10 -f image2 -s 1920x1080 -i /content/inferred_videos/%04d.png -vcodec libx264 -crf 25  -pix_fmt yuv420p test.mp4\n"
      ],
      "metadata": {
        "id": "kByeRFIYlant",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "00fca705-860b-4358-b4d7-fdc437b8d69d"
      },
      "execution_count": 119,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/inferred_videos\n",
            "ffmpeg version 4.3.2-0york0~18.04 Copyright (c) 2000-2021 the FFmpeg developers\n",
            "  built with gcc 7 (Ubuntu 7.5.0-3ubuntu1~18.04)\n",
            "  configuration: --prefix=/usr --extra-version='0york0~18.04' --toolchain=hardened --libdir=/usr/lib/x86_64-linux-gnu --incdir=/usr/include/x86_64-linux-gnu --arch=amd64 --enable-gpl --disable-stripping --enable-avresample --disable-filter=resample --enable-gnutls --enable-ladspa --enable-libaom --enable-libass --enable-libbluray --enable-libbs2b --enable-libcaca --enable-libcdio --enable-libcodec2 --enable-libflite --enable-libfontconfig --enable-libfreetype --enable-libfribidi --enable-libgme --enable-libgsm --enable-libjack --enable-libmp3lame --enable-libmysofa --enable-libopenjpeg --enable-libopenmpt --enable-libopus --enable-libpulse --enable-librabbitmq --enable-librsvg --enable-librubberband --enable-libshine --enable-libsnappy --enable-libsoxr --enable-libspeex --enable-libsrt --enable-libssh --enable-libtheora --enable-libtwolame --enable-libvidstab --enable-libvorbis --enable-libvpx --enable-libwavpack --enable-libwebp --enable-libx265 --enable-libxml2 --enable-libxvid --enable-libzmq --enable-libzvbi --enable-lv2 --enable-omx --enable-openal --enable-opencl --enable-opengl --enable-sdl2 --enable-libzimg --enable-pocketsphinx --enable-libdc1394 --enable-libdrm --enable-libiec61883 --enable-chromaprint --enable-frei0r --enable-libx264 --enable-shared\n",
            "  libavutil      56. 51.100 / 56. 51.100\n",
            "  libavcodec     58. 91.100 / 58. 91.100\n",
            "  libavformat    58. 45.100 / 58. 45.100\n",
            "  libavdevice    58. 10.100 / 58. 10.100\n",
            "  libavfilter     7. 85.100 /  7. 85.100\n",
            "  libavresample   4.  0.  0 /  4.  0.  0\n",
            "  libswscale      5.  7.100 /  5.  7.100\n",
            "  libswresample   3.  7.100 /  3.  7.100\n",
            "  libpostproc    55.  7.100 / 55.  7.100\n",
            "Input #0, image2, from '/content/inferred_videos/%04d.png':\n",
            "  Duration: 00:00:05.70, start: 0.000000, bitrate: N/A\n",
            "    Stream #0:0: Video: png, rgb24(pc), 1920x1080, 10 fps, 10 tbr, 10 tbn, 10 tbc\n",
            "Stream mapping:\n",
            "  Stream #0:0 -> #0:0 (png (native) -> h264 (libx264))\n",
            "Press [q] to stop, [?] for help\n",
            "\u001b[1;36m[libx264 @ 0x5624d97b6400] \u001b[0musing cpu capabilities: MMX2 SSE2Fast SSSE3 SSE4.2 AVX FMA3 BMI2 AVX2\n",
            "\u001b[1;36m[libx264 @ 0x5624d97b6400] \u001b[0mprofile High, level 4.0\n",
            "\u001b[1;36m[libx264 @ 0x5624d97b6400] \u001b[0m264 - core 155 r2917 0a84d98 - H.264/MPEG-4 AVC codec - Copyleft 2003-2018 - http://www.videolan.org/x264.html - options: cabac=1 ref=3 deblock=1:0:0 analyse=0x3:0x113 me=hex subme=7 psy=1 psy_rd=1.00:0.00 mixed_ref=1 me_range=16 chroma_me=1 trellis=1 8x8dct=1 cqm=0 deadzone=21,11 fast_pskip=1 chroma_qp_offset=-2 threads=3 lookahead_threads=1 sliced_threads=0 nr=0 decimate=1 interlaced=0 bluray_compat=0 constrained_intra=0 bframes=3 b_pyramid=2 b_adapt=1 b_bias=0 direct=1 weightb=1 open_gop=0 weightp=2 keyint=250 keyint_min=10 scenecut=40 intra_refresh=0 rc_lookahead=40 rc=crf mbtree=1 crf=25.0 qcomp=0.60 qpmin=0 qpmax=69 qpstep=4 ip_ratio=1.40 aq=1:1.00\n",
            "Output #0, mp4, to 'test.mp4':\n",
            "  Metadata:\n",
            "    encoder         : Lavf58.45.100\n",
            "    Stream #0:0: Video: h264 (libx264) (avc1 / 0x31637661), yuv420p, 1920x1080, q=-1--1, 10 fps, 10240 tbn, 10 tbc\n",
            "    Metadata:\n",
            "      encoder         : Lavc58.91.100 libx264\n",
            "    Side data:\n",
            "      cpb: bitrate max/min/avg: 0/0/0 buffer size: 0 vbv_delay: N/A\n",
            "frame=   57 fps=7.2 q=-1.0 Lsize=     915kB time=00:00:05.40 bitrate=1388.0kbits/s speed=0.684x    \n",
            "video:913kB audio:0kB subtitle:0kB other streams:0kB global headers:0kB muxing overhead: 0.162921%\n",
            "\u001b[1;36m[libx264 @ 0x5624d97b6400] \u001b[0mframe I:1     Avg QP:17.10  size:104256\n",
            "\u001b[1;36m[libx264 @ 0x5624d97b6400] \u001b[0mframe P:14    Avg QP:18.22  size: 34630\n",
            "\u001b[1;36m[libx264 @ 0x5624d97b6400] \u001b[0mframe B:42    Avg QP:20.72  size:  8230\n",
            "\u001b[1;36m[libx264 @ 0x5624d97b6400] \u001b[0mconsecutive B-frames:  1.8%  0.0%  0.0% 98.2%\n",
            "\u001b[1;36m[libx264 @ 0x5624d97b6400] \u001b[0mmb I  I16..4: 43.6% 33.4% 23.1%\n",
            "\u001b[1;36m[libx264 @ 0x5624d97b6400] \u001b[0mmb P  I16..4:  8.3%  8.3%  1.0%  P16..4: 45.0%  9.9%  5.4%  0.0%  0.0%    skip:22.1%\n",
            "\u001b[1;36m[libx264 @ 0x5624d97b6400] \u001b[0mmb B  I16..4:  0.5%  0.6%  0.2%  B16..8: 32.0%  1.8%  0.3%  direct: 6.6%  skip:58.0%  L0:49.6% L1:48.4% BI: 2.0%\n",
            "\u001b[1;36m[libx264 @ 0x5624d97b6400] \u001b[0m8x8 transform intra:43.7% inter:69.3%\n",
            "\u001b[1;36m[libx264 @ 0x5624d97b6400] \u001b[0mcoded y,uvDC,uvAC intra: 26.0% 76.1% 28.8% inter: 6.1% 25.2% 0.7%\n",
            "\u001b[1;36m[libx264 @ 0x5624d97b6400] \u001b[0mi16 v,h,dc,p: 33% 37% 15% 15%\n",
            "\u001b[1;36m[libx264 @ 0x5624d97b6400] \u001b[0mi8 v,h,dc,ddl,ddr,vr,hd,vl,hu: 18% 26% 43%  2%  1%  2%  2%  2%  4%\n",
            "\u001b[1;36m[libx264 @ 0x5624d97b6400] \u001b[0mi4 v,h,dc,ddl,ddr,vr,hd,vl,hu: 25% 25% 17%  4%  5%  5%  5%  5%  9%\n",
            "\u001b[1;36m[libx264 @ 0x5624d97b6400] \u001b[0mi8c dc,h,v,p: 40% 34% 20%  7%\n",
            "\u001b[1;36m[libx264 @ 0x5624d97b6400] \u001b[0mWeighted P-Frames: Y:0.0% UV:0.0%\n",
            "\u001b[1;36m[libx264 @ 0x5624d97b6400] \u001b[0mref P L0: 64.6%  4.9% 21.5%  9.0%\n",
            "\u001b[1;36m[libx264 @ 0x5624d97b6400] \u001b[0mref B L0: 86.4% 10.7%  2.9%\n",
            "\u001b[1;36m[libx264 @ 0x5624d97b6400] \u001b[0mref B L1: 95.2%  4.8%\n",
            "\u001b[1;36m[libx264 @ 0x5624d97b6400] \u001b[0mkb/s:1311.91\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- TODO: add garbage collector instructions here for repeat testing"
      ],
      "metadata": {
        "id": "Pdf500q3zkBv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# CAUTION - deletes all data from the `inferred_videos` and `videos_to_infer` directories\n",
        "# use this code block between runs for a fresh start between videos.\n",
        "%cd /content/\n",
        "!rm -r inferred_videos\n",
        "!rm -r videos_to_infer\n",
        "!mkdir inferred_videos\n",
        "!mkdir videos_to_infer"
      ],
      "metadata": {
        "id": "ojgPEUh4vdCI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "- TODO - util features such as print list of all unique classe?\n",
        "- TODO - update docs in notebooks\n",
        "- TODO - update test cases"
      ],
      "metadata": {
        "id": "ed_3963djk3w"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GJZeiSo_TaSb"
      },
      "source": [
        "# Tested Cases\n",
        "\n",
        "- no images exist\n",
        "- image has no detections\n",
        "- image has no target_detection instance\n",
        "- image has less object count\n",
        "- image has less class count\n",
        "- prediction falls into confidence range\n",
        "- prediction outside of confidence range\n",
        "- prediction has less than box req\n",
        "- prediction has greater than box req\n",
        "- prediction doesn't match target_detection name\n",
        "- first prediction meets requirements for upload\n",
        "- last prediction meets requirements for upload\n",
        "- all similarities match too high, even when images look drastically different"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "MFF8tkyK70_D"
      ],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}